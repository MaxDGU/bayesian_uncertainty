{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218bd802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCVI Import\n",
    "import sys\n",
    "#if branch is stable, will install via pypi, else will install from source\n",
    "branch = \"stable\"\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB and branch == \"stable\":\n",
    "    !pip3 install --quiet scvi-tools[tutorials]\n",
    "elif IN_COLAB and branch != \"stable\":\n",
    "    !pip3 install --quiet --upgrade jsonschema\n",
    "    !pip3 install --quiet git+https://github.com/yoseflab/scvi-tools@$branch#egg=scvi-tools[tutorials]\n",
    "\n",
    "#batchensemble import\n",
    "import scvi\n",
    "import scanpy as sc\n",
    "import os\n",
    "import time\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import robustness_metrics as rm\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import uncertainty_baselines as ub\n",
    "import utils  # local file import\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "sc.set_figure_params(figsize=(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd1d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading\n",
    "adata = scvi.data.heart_cell_atlas_subsampled()\n",
    "sc.pp.filter_genes(adata, min_counts=3)\n",
    "adata.layers[\"counts\"] = adata.X.copy() # preserve counts\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "adata.raw = adata # freeze the state in `.raw`\n",
    "\n",
    "sc.pp.highly_variable_genes(\n",
    "    adata, \n",
    "    n_top_genes=1200, \n",
    "    subset=True, \n",
    "    layer=\"counts\", \n",
    "    flavor=\"seurat_v3\",\n",
    "    batch_key=\"cell_source\"\n",
    ")\n",
    "\n",
    "scvi.data.setup_anndata(\n",
    "    adata,\n",
    "    layer=\"counts\",\n",
    "    categorical_covariate_keys=[\"cell_source\", \"donor\"],\n",
    "    continuous_covariate_keys=[\"percent_mito\", \"percent_ribo\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d344af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = scvi.model.SCVI(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aace9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('ensemble_size', 4, 'Size of ensemble.')\n",
    "flags.DEFINE_float('random_sign_init', -0.5,\n",
    "                   'Use random sign init for fast weights.')\n",
    "flags.DEFINE_float('fast_weight_lr_multiplier', 0.5,\n",
    "                   'fast weights lr multiplier.')\n",
    "\n",
    "\n",
    "# Redefining default values\n",
    "flags.FLAGS.set_default('l2', 3e-4)\n",
    "flags.FLAGS.set_default('lr_decay_epochs', ['80', '160', '180'])\n",
    "flags.FLAGS.set_default('train_epochs', 250)\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"training steps in this method need to be modified\n",
    "to adjust for adata types from SCVI\n",
    "\"\"\"\"\n",
    "def main(argv):\n",
    "  del argv  # unused arg\n",
    "  tf.io.gfile.makedirs(FLAGS.output_dir)\n",
    "  logging.info('Saving checkpoints at %s', FLAGS.output_dir)\n",
    "  tf.random.set_seed(FLAGS.seed)\n",
    "\n",
    "  per_core_batch_size = FLAGS.per_core_batch_size // FLAGS.ensemble_size\n",
    "  batch_size = per_core_batch_size * FLAGS.num_cores\n",
    "\n",
    "  data_dir = FLAGS.data_dir\n",
    "  if FLAGS.use_gpu:\n",
    "    logging.info('Use GPU')\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "  else:\n",
    "    logging.info('Use TPU at %s',\n",
    "                 FLAGS.tpu if FLAGS.tpu is not None else 'local')\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=FLAGS.tpu)\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "\n",
    "  train_builder = ub.datasets.get(\n",
    "      FLAGS.dataset,\n",
    "      data_dir=data_dir,\n",
    "      download_data=FLAGS.download_data,\n",
    "      split=tfds.Split.TRAIN,\n",
    "      validation_percent=1. - FLAGS.train_proportion)\n",
    "  train_dataset = train_builder.load(batch_size=batch_size)\n",
    "  train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "\n",
    "  validation_dataset = None\n",
    "  steps_per_validation = 0\n",
    "  if FLAGS.train_proportion < 1.0:\n",
    "    validation_builder = ub.datasets.get(\n",
    "        FLAGS.dataset,\n",
    "        data_dir=data_dir,\n",
    "        split=tfds.Split.VALIDATION,\n",
    "        validation_percent=1. - FLAGS.train_proportion)\n",
    "    validation_dataset = validation_builder.load(batch_size=batch_size)\n",
    "    validation_dataset = strategy.experimental_distribute_dataset(\n",
    "        validation_dataset)\n",
    "    steps_per_validation = validation_builder.num_examples // batch_size\n",
    "\n",
    "  clean_test_builder = ub.datasets.get(\n",
    "      FLAGS.dataset,\n",
    "      data_dir=data_dir,\n",
    "      split=tfds.Split.TEST)\n",
    "  clean_test_dataset = clean_test_builder.load(batch_size=batch_size)\n",
    "  test_datasets = {\n",
    "      'clean': strategy.experimental_distribute_dataset(clean_test_dataset),\n",
    "  }\n",
    "  steps_per_epoch = train_builder.num_examples // batch_size\n",
    "  steps_per_eval = clean_test_builder.num_examples // batch_size\n",
    "  num_classes = 100 if FLAGS.dataset == 'cifar100' else 10\n",
    "  if FLAGS.corruptions_interval > 0:\n",
    "    if FLAGS.dataset == 'cifar100':\n",
    "      data_dir = FLAGS.cifar100_c_path\n",
    "    corruption_types, _ = utils.load_corrupted_test_info(FLAGS.dataset)\n",
    "    for corruption_type in corruption_types:\n",
    "      for severity in range(1, 6):\n",
    "        dataset = ub.datasets.get(\n",
    "            f'{FLAGS.dataset}_corrupted',\n",
    "            corruption_type=corruption_type,\n",
    "            data_dir=data_dir,\n",
    "            severity=severity,\n",
    "            split=tfds.Split.TEST).load(batch_size=batch_size)\n",
    "        test_datasets[f'{corruption_type}_{severity}'] = (\n",
    "            strategy.experimental_distribute_dataset(dataset))\n",
    "\n",
    "  summary_writer = tf.summary.create_file_writer(\n",
    "      os.path.join(FLAGS.output_dir, 'summaries'))\n",
    "\n",
    "  with strategy.scope():\n",
    "    logging.info('Building SCVI model')\n",
    "    epochs = 10\n",
    "    latent_dim = 2\n",
    "    num_examples_to_generate = 16\n",
    "    random_vector_for_generation = tf.random.normal(\n",
    "        shape=[num_examples_to_generate, latent_dim])\n",
    "    model = scvi.model.SCVI(adata) #imported SCVI\n",
    "    logging.info('Model input shape: %s', model.input_shape)\n",
    "    logging.info('Model output shape: %s', model.output_shape)\n",
    "    logging.info('Model number of weights: %s', model.count_params())\n",
    "    # Linearly scale learning rate and the decay epochs by vanilla settings.\n",
    "    base_lr = FLAGS.base_learning_rate * batch_size / 128\n",
    "    lr_decay_epochs = [(int(start_epoch_str) * FLAGS.train_epochs) // 200\n",
    "                       for start_epoch_str in FLAGS.lr_decay_epochs]\n",
    "    lr_schedule = ub.schedules.WarmUpPiecewiseConstantSchedule(\n",
    "        steps_per_epoch,\n",
    "        base_lr,\n",
    "        decay_ratio=FLAGS.lr_decay_ratio,\n",
    "        decay_epochs=lr_decay_epochs,\n",
    "        warmup_epochs=FLAGS.lr_warmup_epochs)\n",
    "    optimizer = tf.keras.optimizers.SGD(lr_schedule,\n",
    "                                        momentum=1.0 - FLAGS.one_minus_momentum,\n",
    "                                        nesterov=True)\n",
    "    metrics = {\n",
    "        'train/negative_log_likelihood': tf.keras.metrics.Mean(),\n",
    "        'train/accuracy': tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "        'train/loss': tf.keras.metrics.Mean(),\n",
    "        'train/ece': rm.metrics.ExpectedCalibrationError(\n",
    "            num_bins=FLAGS.num_bins),\n",
    "        'test/negative_log_likelihood': tf.keras.metrics.Mean(),\n",
    "        'test/accuracy': tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "        'test/ece': rm.metrics.ExpectedCalibrationError(\n",
    "            num_bins=FLAGS.num_bins),\n",
    "    }\n",
    "    eval_dataset_splits = ['test']\n",
    "    if validation_dataset:\n",
    "      metrics.update({\n",
    "          'validation/negative_log_likelihood': tf.keras.metrics.Mean(),\n",
    "          'validation/accuracy': tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "          'validation/ece': rm.metrics.ExpectedCalibrationError(\n",
    "              num_bins=FLAGS.num_bins),\n",
    "      })\n",
    "      eval_dataset_splits += ['validation']\n",
    "    for i in range(FLAGS.ensemble_size):\n",
    "      for dataset_split in eval_dataset_splits:\n",
    "        metrics[f'{dataset_split}/nll_member_{i}'] = tf.keras.metrics.Mean()\n",
    "        metrics[f'{dataset_split}/accuracy_member_{i}'] = (\n",
    "            tf.keras.metrics.SparseCategoricalAccuracy())\n",
    "    if FLAGS.corruptions_interval > 0:\n",
    "      corrupt_metrics = {}\n",
    "      for intensity in range(1, 6):\n",
    "        for corruption in corruption_types:\n",
    "          dataset_name = '{0}_{1}'.format(corruption, intensity)\n",
    "          corrupt_metrics['test/nll_{}'.format(dataset_name)] = (\n",
    "              tf.keras.metrics.Mean())\n",
    "          corrupt_metrics['test/accuracy_{}'.format(dataset_name)] = (\n",
    "              tf.keras.metrics.SparseCategoricalAccuracy())\n",
    "          corrupt_metrics['test/ece_{}'.format(dataset_name)] = (\n",
    "              rm.metrics.ExpectedCalibrationError(num_bins=FLAGS.num_bins))\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(FLAGS.output_dir)\n",
    "    initial_epoch = 0\n",
    "    if latest_checkpoint:\n",
    "      # checkpoint.restore must be within a strategy.scope() so that optimizer\n",
    "      # slot variables are mirrored.\n",
    "      checkpoint.restore(latest_checkpoint)\n",
    "      logging.info('Loaded checkpoint %s', latest_checkpoint)\n",
    "      initial_epoch = optimizer.iterations.numpy() // steps_per_epoch\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(iterator):\n",
    "    \"\"\"Training StepFn.\"\"\"\n",
    "    def step_fn(inputs):\n",
    "      \"\"\"Per-Replica StepFn.\"\"\"\n",
    "      images = inputs['features']\n",
    "      labels = inputs['labels']\n",
    "      images = tf.tile(images, [FLAGS.ensemble_size, 1, 1, 1])\n",
    "      labels = tf.tile(labels, [FLAGS.ensemble_size])\n",
    "\n",
    "      with tf.GradientTape() as tape:\n",
    "        logits = model(images, training=True)\n",
    "        negative_log_likelihood = tf.reduce_mean(\n",
    "            tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
    "                                                            logits,\n",
    "                                                            from_logits=True))\n",
    "        l2_loss = sum(model.losses)\n",
    "        loss = negative_log_likelihood + l2_loss\n",
    "        # Scale the loss given the TPUStrategy will reduce sum all gradients.\n",
    "        scaled_loss = loss / strategy.num_replicas_in_sync\n",
    "\n",
    "      grads = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "\n",
    "      # Separate learning rate implementation.\n",
    "      if FLAGS.fast_weight_lr_multiplier != 1.0:\n",
    "        grads_and_vars = []\n",
    "        for grad, var in zip(grads, model.trainable_variables):\n",
    "          # Apply different learning rate on the fast weight approximate\n",
    "          # posterior/prior parameters. This is excludes BN and slow weights,\n",
    "          # but pay caution to the naming scheme.\n",
    "          if ('batch_norm' not in var.name and 'kernel' not in var.name):\n",
    "            grads_and_vars.append((grad * FLAGS.fast_weight_lr_multiplier, var))\n",
    "          else:\n",
    "            grads_and_vars.append((grad, var))\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "      else:\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      probs = tf.nn.softmax(logits)\n",
    "      metrics['train/ece'].add_batch(probs, label=labels)\n",
    "      metrics['train/loss'].update_state(loss)\n",
    "      metrics['train/negative_log_likelihood'].update_state(\n",
    "          negative_log_likelihood)\n",
    "      metrics['train/accuracy'].update_state(labels, logits)\n",
    "\n",
    "    for _ in tf.range(tf.cast(steps_per_epoch, tf.int32)):\n",
    "      strategy.run(step_fn, args=(next(iterator),))\n",
    "\n",
    "  @tf.function\n",
    "  def test_step(iterator, dataset_split, dataset_name, num_steps):\n",
    "    \"\"\"Evaluation StepFn.\"\"\"\n",
    "    def step_fn(inputs):\n",
    "      \"\"\"Per-Replica StepFn.\"\"\"\n",
    "      images = inputs['features']\n",
    "      labels = inputs['labels']\n",
    "      images = tf.tile(images, [FLAGS.ensemble_size, 1, 1, 1])\n",
    "      logits = model(images, training=False)\n",
    "      probs = tf.nn.softmax(logits)\n",
    "      per_probs = tf.split(probs,\n",
    "                           num_or_size_splits=FLAGS.ensemble_size,\n",
    "                           axis=0)\n",
    "      for i in range(FLAGS.ensemble_size):\n",
    "        member_probs = per_probs[i]\n",
    "        member_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            labels, member_probs)\n",
    "        metrics[f'{dataset_split}/nll_member_{i}'].update_state(member_loss)\n",
    "        metrics[f'{dataset_split}/accuracy_member_{i}'].update_state(\n",
    "            labels, member_probs)\n",
    "\n",
    "      probs = tf.reduce_mean(per_probs, axis=0)\n",
    "      negative_log_likelihood = tf.reduce_mean(\n",
    "          tf.keras.losses.sparse_categorical_crossentropy(labels, probs))\n",
    "      if dataset_name == 'clean':\n",
    "        metrics[f'{dataset_split}/negative_log_likelihood'].update_state(\n",
    "            negative_log_likelihood)\n",
    "        metrics[f'{dataset_split}/accuracy'].update_state(labels, probs)\n",
    "        metrics[f'{dataset_split}/ece'].add_batch(probs, label=labels)\n",
    "      else:\n",
    "        corrupt_metrics['test/nll_{}'.format(dataset_name)].update_state(\n",
    "            negative_log_likelihood)\n",
    "        corrupt_metrics['test/accuracy_{}'.format(dataset_name)].update_state(\n",
    "            labels, probs)\n",
    "        corrupt_metrics['test/ece_{}'.format(dataset_name)].add_batch(\n",
    "            probs, label=labels)\n",
    "\n",
    "    for _ in tf.range(tf.cast(num_steps, tf.int32)):\n",
    "      strategy.run(step_fn, args=(next(iterator),))\n",
    "\n",
    "  metrics.update({'test/ms_per_example': tf.keras.metrics.Mean()})\n",
    "\n",
    "  train_iterator = iter(train_dataset)\n",
    "  start_time = time.time()\n",
    "  for epoch in range(initial_epoch, FLAGS.train_epochs):\n",
    "    logging.info('Starting to run epoch: %s', epoch)\n",
    "    train_step(train_iterator)\n",
    "\n",
    "    current_step = (epoch + 1) * steps_per_epoch\n",
    "    max_steps = steps_per_epoch * FLAGS.train_epochs\n",
    "    time_elapsed = time.time() - start_time\n",
    "    steps_per_sec = float(current_step) / time_elapsed\n",
    "    eta_seconds = (max_steps - current_step) / steps_per_sec\n",
    "    message = ('{:.1%} completion: epoch {:d}/{:d}. {:.1f} steps/s. '\n",
    "               'ETA: {:.0f} min. Time elapsed: {:.0f} min'.format(\n",
    "                   current_step / max_steps,\n",
    "                   epoch + 1,\n",
    "                   FLAGS.train_epochs,\n",
    "                   steps_per_sec,\n",
    "                   eta_seconds / 60,\n",
    "                   time_elapsed / 60))\n",
    "    logging.info(message)\n",
    "\n",
    "    if validation_dataset:\n",
    "      validation_iterator = iter(validation_dataset)\n",
    "      test_step(\n",
    "          validation_iterator, 'validation', 'clean', steps_per_validation)\n",
    "    datasets_to_evaluate = {'clean': test_datasets['clean']}\n",
    "    if (FLAGS.corruptions_interval > 0 and\n",
    "        (epoch + 1) % FLAGS.corruptions_interval == 0):\n",
    "      datasets_to_evaluate = test_datasets\n",
    "    for dataset_name, test_dataset in datasets_to_evaluate.items():\n",
    "      test_iterator = iter(test_dataset)\n",
    "      logging.info('Testing on dataset %s', dataset_name)\n",
    "      logging.info('Starting to run eval at epoch: %s', epoch)\n",
    "      test_start_time = time.time()\n",
    "      test_step(test_iterator, 'test', dataset_name, steps_per_eval)\n",
    "      ms_per_example = (time.time() - test_start_time) * 1e6 / batch_size\n",
    "      metrics['test/ms_per_example'].update_state(ms_per_example)\n",
    "\n",
    "      logging.info('Done with testing on %s', dataset_name)\n",
    "\n",
    "    corrupt_results = {}\n",
    "    if (FLAGS.corruptions_interval > 0 and\n",
    "        (epoch + 1) % FLAGS.corruptions_interval == 0):\n",
    "      corrupt_results = utils.aggregate_corrupt_metrics(corrupt_metrics,\n",
    "                                                        corruption_types)\n",
    "\n",
    "    logging.info('Train Loss: %.4f, Accuracy: %.2f%%',\n",
    "                 metrics['train/loss'].result(),\n",
    "                 metrics['train/accuracy'].result() * 100)\n",
    "    logging.info('Test NLL: %.4f, Accuracy: %.2f%%',\n",
    "                 metrics['test/negative_log_likelihood'].result(),\n",
    "                 metrics['test/accuracy'].result() * 100)\n",
    "    for i in range(FLAGS.ensemble_size):\n",
    "      logging.info('Member %d Test Loss: %.4f, Accuracy: %.2f%%',\n",
    "                   i, metrics['test/nll_member_{}'.format(i)].result(),\n",
    "                   metrics['test/accuracy_member_{}'.format(i)].result() * 100)\n",
    "    total_results = {name: metric.result() for name, metric in metrics.items()}\n",
    "    total_results.update(corrupt_results)\n",
    "    # Metrics from Robustness Metrics (like ECE) will return a dict with a\n",
    "    # single key/value, instead of a scalar.\n",
    "    total_results = {\n",
    "        k: (list(v.values())[0] if isinstance(v, dict) else v)\n",
    "        for k, v in total_results.items()\n",
    "    }\n",
    "    with summary_writer.as_default():\n",
    "      for name, result in total_results.items():\n",
    "        tf.summary.scalar(name, result, step=epoch + 1)\n",
    "\n",
    "    for metric in metrics.values():\n",
    "      metric.reset_states()\n",
    "\n",
    "    if (FLAGS.checkpoint_interval > 0 and\n",
    "        (epoch + 1) % FLAGS.checkpoint_interval == 0):\n",
    "      checkpoint_name = checkpoint.save(\n",
    "          os.path.join(FLAGS.output_dir, 'checkpoint'))\n",
    "      logging.info('Saved checkpoint to %s', checkpoint_name)\n",
    "\n",
    "  final_checkpoint_name = checkpoint.save(\n",
    "      os.path.join(FLAGS.output_dir, 'checkpoint'))\n",
    "  logging.info('Saved last checkpoint to %s', final_checkpoint_name)\n",
    "  with summary_writer.as_default():\n",
    "    hp.hparams({\n",
    "        'base_learning_rate': FLAGS.base_learning_rate,\n",
    "        'one_minus_momentum': FLAGS.one_minus_momentum,\n",
    "        'l2': FLAGS.l2,\n",
    "        'random_sign_init': FLAGS.random_sign_init,\n",
    "        'fast_weight_lr_multiplier': FLAGS.fast_weight_lr_multiplier,\n",
    "    })\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  app.run(main)\n",
    "\n",
    "\"\"\"\"\n",
    "Functionality to be added to track \n",
    "training performance and accuracy\n",
    "across ensemble and between members\n",
    "\"\"\"\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
